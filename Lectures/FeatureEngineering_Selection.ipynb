{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CEE 690-02\n",
    "\n",
    "# Environmental Spatial Data Analysis\n",
    "\n",
    "# Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dataset: EUROSAT\n",
    "\n",
    "![image.png](https://github.com/phelber/EuroSAT/raw/master/eurosat_overview_small.jpg?raw=true)\n",
    "\n",
    "* 27,000 labeled and geo-referenced images with 13 spectral bands from Sentinel-2\n",
    "* Each image covers a 640 m by 640 m domain at a 10 meter spatial resolution. \n",
    "* The images are a collection of sites around Europe\n",
    "* Each image has been prelabeled a given land use class (e.g., Forest).\n",
    "* This dataset can be used to construct models to predict land use from Sentinel-2 data\n",
    "\n",
    "For more info go here: https://github.com/phelber/eurosat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we go any further, it's time we learn a little bit of basic remote sensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-2 \n",
    "\n",
    "![image.png](https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2011/11/sentinel-2/10202548-2-eng-GB/Sentinel-2_pillars.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Video time!\n",
    "\n",
    "https://www.youtube.com/watch?v=Bv3pB9TaWOk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentinel-2 spectral bands\n",
    "\n",
    "![image.png](https://eo4geo.sbg.ac.at/IGIK/Sentinel2_Data_and_Vegetation_Indices/S2-2020_ESA.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "db = pickle.load(open('/data/ds/images/remote_sensing/otherDatasets/sentinel_2_subset.pck','rb'))\n",
    "y = db['obs']['lc_id'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see what the data looks like for a given domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in db['covariates']:\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(var,fontsize=50)\n",
    "    plt.imshow(db['covariates'][var][150,:,:])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start working with our data! The objective is to fit the best classification model for the data. \n",
    "\n",
    "We will use random forests today; our focus is on what we call \"feature engineering\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So what would you do with all that data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spatial means\n",
    "\n",
    "Let's start with the simplest thing and use the spatial mean of each Sentinel-2 band at each site as a predictor. So this leads to 13 predictors per site. Note that the number of sites has been reduced to 1000 to avoid memory issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Spatial means dataset\n",
    "X = []\n",
    "for var in db['covariates']:\n",
    "    print(db['covariates'][var].shape)\n",
    "    X.append(np.mean(db['covariates'][var],axis=(1,2)))\n",
    "X = np.array(X).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spatial means: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 0.325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "clf = RandomForestClassifier(random_state=0,n_estimators=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Classification accuracy:',clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What can we do to improve our accuracy score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spatial means + spatial variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 64, 64)\n",
      "(1000, 64, 64)\n",
      "(1000, 64, 64)\n",
      "(1000, 64, 64)\n",
      "(1000, 64, 64)\n",
      "(1000, 64, 64)\n",
      "(1000, 64, 64)\n",
      "(1000, 64, 64)\n",
      "(1000, 64, 64)\n",
      "(1000, 64, 64)\n",
      "(1000, 64, 64)\n",
      "(1000, 64, 64)\n",
      "(1000, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "#Spatial means + variances dataset\n",
    "X = []\n",
    "for var in db['covariates']:\n",
    "    print(db['covariates'][var].shape)\n",
    "    X.append(np.mean(db['covariates'][var],axis=(1,2)))\n",
    "    X.append(np.var(db['covariates'][var],axis=(1,2)))\n",
    "X = np.array(X).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spatial means + spatial variances: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 0.815\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=0,n_estimators=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Classification accuracy:',clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spatial means + spatial variances + spatial medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatial means + variances + median dataset\n",
    "X = []\n",
    "for var in db['covariates']:\n",
    "    print(db['covariates'][var].shape)\n",
    "    X.append(np.mean(db['covariates'][var],axis=(1,2)))\n",
    "    X.append(np.var(db['covariates'][var],axis=(1,2)))\n",
    "    X.append(np.percentile(db['covariates'][var],50,axis=(1,2)))\n",
    "X = np.array(X).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spatial means + spatial variances + spatial medians: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0,n_estimators=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Classification accuracy:',clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what just happened? Why did it go down?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest: Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(clf.feature_importances_,lw=4)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features (or predictors) matter, and some don't. Throwing more and more features at a model doesn't ensure improved performance. Many times it will actually degrade performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What can we do to add more useful features?\n",
    "\n",
    "There are actually a lot of options..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More features: Add spectral indices\n",
    "\n",
    "These effectively combine the spectral bands from the satellite to arrive at indices that are higher quality than any given band and are more strongly connected to the problem at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spectral vegetation signatures\n",
    "\n",
    "![image.png](https://eo4geo.sbg.ac.at/IGIK/Sentinel2_Data_and_Vegetation_Indices/leaf_reflectance.png)\n",
    "\n",
    "Vegetation spectra correspond to bundles of leaves and steams of Spartina alternifora, a wetland perennial grass from Kokaly et al. (2017), Soil spectrum from Clark (1999). Figure adapted by Denis, A. (2018) from Kokaly et al. (1998), Bowker et al. (1985), Curran (1989) and Thenkabail et al. (2013). Source of image: https://eo4geo.sbg.ac.at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Seen another way...\n",
    "\n",
    "![image.png](https://eo4geo.sbg.ac.at/IGIK/Sentinel2_Data_and_Vegetation_Indices/plant_spectral_properties.jpeg)\n",
    "\n",
    "Varied response of a given plant depending on the time of year. Source of image: https://eo4geo.sbg.ac.at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral indices harness the spectral signatures from different land cover types to determine vegetation status, and land cover types, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's review some spectral indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RVI (Ratio vegetation index)\n",
    "\n",
    "$$ \\text{RVI} = \\frac{\\text{NIR}}{\\text{RED}}$$\n",
    "\n",
    "This is an example of a band ratio which is a common practice to remove effects of topography and atmospheric effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIR = db['covariates']['b8']\n",
    "RED = db['covariates']['b4']\n",
    "RVI = (NIR)/(RED)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(RVI[150,:,:])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NDVI (Normalized difference vegetation index)\n",
    "\n",
    "$$ \\text{NDVI} = \\frac{\\text{NIR} - \\text{RED}}{\\text{NIR} + \\text{RED}}$$\n",
    "\n",
    "![image.png](https://eo4geo.sbg.ac.at/IGIK/Sentinel2_Data_and_Vegetation_Indices/NDVI_trees.png)\n",
    "\n",
    "NDVI is effective for quantifying green vegetation. Positively correlated with vegetation greenness. There are multiple variations on NDVI such as TNDVI or NDI45.\n",
    "\n",
    "Source: Wu Ch-D., et al. (2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIR = db['covariates']['b8']\n",
    "RED = db['covariates']['b4']\n",
    "NDVI = (NIR-RED)/(NIR+RED)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(NDVI[150,:,:])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GNDVI (Green normalized difference vegetation index)\n",
    "\n",
    "$$ \\text{GNDVI} = \\frac{\\text{NIR} - \\text{GREEN}}{\\text{NIR} + \\text{GREEN}}$$\n",
    "\n",
    "GNDVI is more sensitive than NDVI to different concentration rates of chlorophyll, which is highly correlated to nitrogen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIR = db['covariates']['b8']\n",
    "GREEN = db['covariates']['b3']\n",
    "GNDVI = (NIR-GREEN)/(NIR+GREEN)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(GNDVI[150,:,:])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IRECI (Inverted Red-Edge Chlorophyll Index)\n",
    "\n",
    "$$ \\text{IRECI} = \\frac{\\text{NIR} - \\text{RED1}}{\\text{RED2} + \\text{RED3}}$$\n",
    "\n",
    "IRECI incorporates the reflectance in four bands to estimate canopy chlorophyll content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIR = db['covariates']['b8']\n",
    "RED1 = db['covariates']['b5']\n",
    "RED2 = db['covariates']['b6']\n",
    "RED3 = db['covariates']['b7']\n",
    "IRECI = (NIR-RED1)/(RED2+RED3)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(IRECI[150,:,:])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SAVI (Soil Adjusted Vegetation Index)\n",
    "\n",
    "$$ \\text{SAVI} = 1.428\\frac{\\text{NIR} - \\text{RED}}{NIR + RED + 0.428} $$\n",
    "\n",
    "SAVI uses a transformation technique that minimizes soil brightness influences from spectral vegetation indices involving red and near-infrared (NIR) wavelengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIR = db['covariates']['b8']\n",
    "RED = db['covariates']['b5']\n",
    "SAVI = 1.428*(NIR-RED)/(NIR + RED + 0.428)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(SAVI[150,:,:])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NDMI (Normalized Difference Water Index)\n",
    "\n",
    "$$ \\text{NDMI} = \\frac{\\text{NIR} - \\text{SWIR}}{\\text{NIR} + \\text{SWIR}}$$\n",
    "\n",
    "NDMI describes the cropâ€™s water stress level and is calculated as the ratio between the difference and the sum of the refracted radiations in the near infrared and SWIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIR = db['covariates']['b8']\n",
    "SWIR = db['covariates']['b11']\n",
    "NDMI = (NIR-SWIR)/(NIR+SWIR)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(NDMI[150,:,:])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# I think you get the point..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensors on a satellite dictate what indices we can use. Don't fixate too much on what each index means. Sentinel-2 just has a lot of bands which makes it possible to do a lot more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe width=\"939\" height=\"528\" src=\"https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/indexdb/\" frameborder=\"0\" allow=\"accelerometer;  autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's now add all these indices as our covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatial means + variances + median dataset of everything\n",
    "db['derived_covariates'] = {'NDMI':NDMI,'SAVI':SAVI,'IRECI':IRECI,'GNDVI':GNDVI,'NDVI':NDVI}\n",
    "X = []\n",
    "for var in db['derived_covariates']:\n",
    "    print(db['derived_covariates'][var].shape)\n",
    "    X.append(np.mean(db['derived_covariates'][var],axis=(1,2)))\n",
    "    X.append(np.var(db['derived_covariates'][var],axis=(1,2)))\n",
    "    X.append(np.percentile(db['derived_covariates'][var],50,axis=(1,2)))\n",
    "for var in db['covariates']:\n",
    "    print(db['covariates'][var].shape)\n",
    "    X.append(np.mean(db['covariates'][var],axis=(1,2)))\n",
    "    X.append(np.var(db['covariates'][var],axis=(1,2)))\n",
    "    X.append(np.percentile(db['covariates'][var],50,axis=(1,2)))\n",
    "X = np.array(X).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0,n_estimators=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Classification accuracy:',clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making progress!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More features: Image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Canny edge detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature\n",
    "NIR = db['covariates']['b8']\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(NIR[150,:,:])\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(feature.canny(NIR[150,:,:],sigma=1))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for var in db['covariates']:\n",
    "    CANNY = []\n",
    "    print(var)\n",
    "    for i in range(db['covariates'][var].shape[0]):\n",
    "        CANNY.append(feature.canny(db['covariates'][var][i,:,:],sigma=1))\n",
    "    db['derived_covariates']['%s_CANNY' % var] = np.array(CANNY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ridge filters\n",
    "\n",
    "Ridge filters can be used to detect ridge-like structures, such as neurites, tubes, vessels, wrinkles or rivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "from skimage.morphology import disk\n",
    "selem = disk(3)\n",
    "NIR = db['covariates']['b8']\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(NIR[150,:,:])\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(filters.sato(NIR[150,:,:]))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for var in db['covariates']:\n",
    "    RIDGE = []\n",
    "    print(var)\n",
    "    for i in range(db['covariates'][var].shape[0]):\n",
    "        RIDGE.append(filters.sato(db['covariates'][var][i,:,:]))\n",
    "    db['derived_covariates']['%s_RIDGE' % var] = np.array(RIDGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mean filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "from skimage.morphology import disk\n",
    "selem = disk(3)\n",
    "NIR = db['covariates']['b8']\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(NIR[150,:,:])\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "tmp = NIR[150,:,:]\n",
    "tmp = (tmp - np.min(tmp))/(np.max(tmp) - np.min(tmp))\n",
    "plt.imshow(filters.rank.mean(tmp,selem=selem))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### And lots, lots more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Add some of these features to the mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatial means + variances + median dataset of everything\n",
    "X = []\n",
    "for var in db['derived_covariates']:\n",
    "    print(db['derived_covariates'][var].shape)\n",
    "    X.append(np.mean(db['derived_covariates'][var],axis=(1,2)))\n",
    "    X.append(np.var(db['derived_covariates'][var],axis=(1,2)))\n",
    "    X.append(np.percentile(db['derived_covariates'][var],50,axis=(1,2)))\n",
    "for var in db['covariates']:\n",
    "    print(db['covariates'][var].shape)\n",
    "    X.append(np.mean(db['covariates'][var],axis=(1,2)))\n",
    "    X.append(np.var(db['covariates'][var],axis=(1,2)))\n",
    "    X.append(np.percentile(db['covariates'][var],50,axis=(1,2)))\n",
    "X = np.array(X).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0,n_estimators=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Classification accuracy:',clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Progress?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(clf.feature_importances_,lw=4)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What's wrong here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recursive Feature Elimination\n",
    "\n",
    "Iteratively fit a model and remove features. Features are removed based on their feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "estimator = RandomForestClassifier(random_state=0,n_estimators=100)\n",
    "clf = RFE(estimator, n_features_to_select=25,step=5,verbose=True)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it didn't do much here beyond reducing the number of features, sometimes it can help with model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality reduction: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "#Standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "Xstd = scaler.transform(X)\n",
    "#Define the parameters\n",
    "pca = sklearn.decomposition.PCA(n_components=100)\n",
    "#Fit the model\n",
    "pca.fit(Xstd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA: Explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explained variance\n",
    "plt.plot(pca.explained_variance_/np.sum(pca.explained_variance_),lw=4)\n",
    "plt.ylabel('Relative explained variance',fontsize=16)\n",
    "plt.xlabel('Principal component',fontsize=16)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's only keep the top 20 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the parameters\n",
    "pca = sklearn.decomposition.PCA(n_components=20)\n",
    "#Fit the model\n",
    "pca.fit(Xstd)\n",
    "#Transform the data\n",
    "PCS = pca.transform(Xstd)\n",
    "#Inverse transform the data\n",
    "Xpred_std = pca.inverse_transform(PCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest on the 20 PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0,n_estimators=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(PCS, y, test_size=0.2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Classification accuracy:',clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest on the inversely transformed PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0,n_estimators=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xpred_std, y, test_size=0.2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Classification accuracy:',clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA effectively de-noised the data and thus improves predictability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions...\n",
    "\n",
    "* We started at an accuracy of 0.7 and ended at 0.865. So it was not a waste of time!\n",
    "* Finding the right features and figuring out how to leverage them can get very tricky. \n",
    "* Word of advice: Don't start massaging your data until you are already in the ballpark of your target accuracies. At the end of the day, if you have the right model and the right features, feature engineering and selection will be a second-order problem. It is fun but it can also lead nowhere.\n",
    "* Wouldn't it be nice if an algorithm did all this feature engineering for me? That is in essence what convolutional neural networks are doing for us (we will talk about it on Thursday). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 64, 64)\n",
      "(1000, 64, 64)\n",
      "(2, 1000, 4096)\n",
      "(1000, 82)\n",
      "Classification accuracy: 0.335\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for var in ['b4','b8',]:#db['covariates']:\n",
    "    print(db['covariates'][var].shape)\n",
    "    X.append(db['covariates'][var].reshape((db['covariates'][var].shape[0],db['covariates'][var].shape[1]*db['covariates'][var].shape[2])))\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "X = X[0,:,::50]\n",
    "print(X.shape)\n",
    "clf = RandomForestClassifier(random_state=0,n_estimators=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Classification accuracy:',clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "rise": "scroll"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
